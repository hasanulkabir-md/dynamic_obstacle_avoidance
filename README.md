---

## ğŸ§± **Folder Structure**

```
dynamic_obstacle_avoidance/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE               # optional (MIT if public)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ dqn_agent.py
â”‚   â”‚   â”œâ”€â”€ double_dqn_agent.py
â”‚   â”‚   â”œâ”€â”€ ppo_agent.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ environment/
â”‚   â”‚   â”œâ”€â”€ gazebo_env.py           # ROS2 interface for obstacle environment
â”‚   â”‚   â”œâ”€â”€ sensors.py              # camera, lidar data parser
â”‚   â”‚   â””â”€â”€ reward_functions.py     # reward shaping for navigation
â”‚   â”‚
â”‚   â”œâ”€â”€ perception/
â”‚   â”‚   â”œâ”€â”€ yolo_detector.py        # object detection with YOLOv5
â”‚   â”‚   â”œâ”€â”€ feature_extractor.py
â”‚   â”‚   â””â”€â”€ tracker.py
â”‚   â”‚
â”‚   â”œâ”€â”€ main_train.py               # training loop (Double DQN)
â”‚   â”œâ”€â”€ evaluate.py                 # evaluation pipeline
â”‚   â”œâ”€â”€ visualize.py                # render training results
â”‚   â””â”€â”€ federated_extension.py      # placeholder for future FedRL research
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”œâ”€â”€ base_config.yaml
â”‚   â”‚   â”œâ”€â”€ yolo_double_dqn.yaml
â”‚   â”‚   â””â”€â”€ adversarial_test.yaml
â”‚   â”‚
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â”œâ”€â”€ run_2025_01/
â”‚   â”‚   â”‚   â”œâ”€â”€ rewards_curve.png
â”‚   â”‚   â”‚   â”œâ”€â”€ success_rate.csv
â”‚   â”‚   â”‚   â””â”€â”€ summary.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”œâ”€â”€ obstacle_avoidance_accuracy.csv
â”‚   â”‚   â””â”€â”€ performance_summary.pdf
â”‚   â”‚
â”‚   â””â”€â”€ notebooks/
â”‚       â”œâ”€â”€ reward_analysis.ipynb
â”‚       â”œâ”€â”€ model_visualization.ipynb
â”‚       â””â”€â”€ adversarial_test.ipynb
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ method_description.pdf
â”‚   â”œâ”€â”€ experiment_summary.md
â”‚   â””â”€â”€ network_architecture.png
â”‚
â””â”€â”€ demo/
    â”œâ”€â”€ demo_video.gif
    â”œâ”€â”€ robot_navigation.mp4
    â””â”€â”€ screenshots/
        â”œâ”€â”€ environment_setup.png
        â”œâ”€â”€ training_reward_curve.png
        â””â”€â”€ navigation_result.png
```

---

## ğŸ§¾ **Sample README.md (you can copy this)**

````markdown
# Video-Based Dynamic Obstacle Avoidance Using Deep Reinforcement Learning

This repository contains the code, experiments, and results for my Master's research project on **video-based dynamic obstacle avoidance** using **Deep Reinforcement Learning (DRL)** at *Nanjing Normal University (Project 211)*.

## ğŸ¯ Research Objective
To develop a real-time video-based obstacle avoidance system that allows mobile robots to navigate dynamic environments by integrating:
- **Computer Vision (YOLOv5)** for obstacle detection
- **Deep Reinforcement Learning (Double DQN, PPO)** for decision-making
- **Gazebo + ROS2 Simulation** for environment testing

## ğŸ§  Method Overview
- **Perception:** YOLOv5 extracts bounding boxes and depth cues from the robotâ€™s camera feed.
- **State Representation:** The vision-based states are fused with positional data.
- **Action Policy:** A Double DQN agent selects motion commands (forward, rotate, stop) to maximize navigation rewards.
- **Reward Design:** Combines distance improvement, collision penalty, and smooth motion incentives.
- **Extension:** Initial experiments on **federated RL** and **adversarial robustness** for secure multi-robot navigation.

<p align="center">
  <img src="docs/network_architecture.png" width="600">
</p>

## âš™ï¸ Dependencies
- Python 3.8+
- PyTorch 2.0+
- OpenAI Gym
- ROS2 Foxy
- Gazebo 11
- YOLOv5 (Ultralytics)
- NumPy, Matplotlib, Tensorboard

Install dependencies:
```bash
pip install -r requirements.txt
````

## ğŸš€ Training and Evaluation

Run training:

```bash
python src/main_train.py --config experiments/configs/yolo_double_dqn.yaml
```

Run evaluation:

```bash
python src/evaluate.py --weights results/best_model.pth
```

## ğŸ“Š Experimental Results

| Metric                 | Value      |
| :--------------------- | :--------- |
| Average Reward         | 264.7      |
| Success Rate           | 92.4%      |
| Average Episode Length | 1500 steps |

<p align="center">
  <img src="experiments/logs/run_2025_01/rewards_curve.png" width="450">
</p>

## ğŸ“¦ Repository Structure

```
src/         # Core source code
experiments/ # Configs, logs, and results
docs/        # Reports and figures
demo/        # Videos and screenshots
```

## ğŸ§© Future Work

This repository also forms the foundation of my proposed PhD work on:

* **Secure Reinforcement Learning**
* **Federated Optimization**
* **Trustworthy Multi-Agent Navigation**

## ğŸ‘¤ Author

**Md. Hasanul Kabir**
Masterâ€™s Student, Computer Science & Technology
Nanjing Normal University, China
ğŸ“§ [hasanul.kabir09@gmail.com](mailto:hasanul.kabir09@gmail.com)
ğŸŒ [Google Scholar / Website](https://sites.google.com/view/md-hasanul-kabir)

```

---
